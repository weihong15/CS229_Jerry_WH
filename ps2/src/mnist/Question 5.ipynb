{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5a\n",
    "\n",
    "We are given that \n",
    "$$CE(y, \\hat{y}) = - \\sum_{k=1}^K y_k \\log \\hat{y_k},$$\n",
    "\n",
    "By multivariate chain rule, we have the following\n",
    "\n",
    "$$ \\nabla_{z^{(i)}} \\mathrm{CE}(y^{(i)}, \\hat{y}^{(i)}) = [\\nabla_{z^{(i)}} \\mathrm{softmax}(z^{(i)})] [\\nabla_{\\hat{y}^{(i)}} \\mathrm{CE}(y^{(i)}, \\hat{y}^{(i)})] $$\n",
    "\n",
    "We will find both terms independently. \n",
    "\n",
    "$$ \\nabla_{\\hat{y}^{(i)}} \\mathrm{CE}(y^{(i)}, \\hat{y}^{(i)}) = \\nabla_{\\hat{y}^{(i)}} [- \\sum_{k=1}^K y_k^{(i)} \\log \\hat{y_k^{(i)}}] = -\\begin{pmatrix}\\frac{y_1^{(i)}}{\\hat{y}_l^{(i)}}\\\\ \\\\ \\frac{y_2^{(i)}}{\\hat{y}_2^{(i)}}\\\\ \\\\ \\vdots \\\\ \\\\ \\frac{y_k^{(i)}}{\\hat{y}_k^{(i)}}\\end{pmatrix}$$\n",
    "\n",
    "the l-th element of the softmax function is given by\n",
    "\n",
    "$$ \\hat{y}_l^{(i)} = \\mathrm{softmax}(z^{(i)}_l) = \\frac{\\exp{(z^{(i)}_l)}}{\\sum_{j=1}^k \\exp{(z^{(i)}_j)}}$$\n",
    "\n",
    "Since all of the terms of the vector have the same form, we only need to consider, $\\frac{\\partial }{\\partial z^{(i)}_l}\\mathrm{softmax}(z^{(i)}_l)$ for the diagonal terms and for the off-diagonal terms $c\\neq l$, $\\frac{\\partial }{\\partial z^{(i)}_c}\\mathrm{softmax}(z^{(i)}_l)$\n",
    "\n",
    "$$ \\frac{\\partial }{\\partial z^{(i)}_l}\\mathrm{softmax}(z^{(i)}_l) = \\frac{\\exp{(z^{(i)}_l)}\\sum_{j=1}^k \\exp{(z^{(i)}_j)} - \\exp{(z^{(i)}_l)}\\exp{(z^{(i)}_l)}}{(\\sum_{j=1}^k \\exp{(z^{(i)}_j)})^2} = \\frac{\\exp{(z^{(i)}_l)}}{\\sum_{j=1}^k \\exp{(z^{(i)}_j)}}\\cdot \\frac{\\sum_{j=1}^k \\exp{(z^{(i)}_j)} - \\exp{(z^{(i)}_l)}}{\\sum_{j=1}^k \\exp{(z^{(i)}_j)}} \\\\ = \\mathrm{softmax}(z^{(i)}_l) \\cdot (1- \\mathrm{softmax}(z^{(i)}_l)) = \\hat{y}_l^{(i)}(1-\\hat{y}_l^{(i)})$$\n",
    "\n",
    "for the off-diagonal terms $c\\neq l$,\n",
    "\n",
    "$$\\frac{\\partial }{\\partial z^{(i)}_c}\\mathrm{softmax}(z^{(i)}_l) = \\exp{(z^{(i)}_l)} \\cdot [-(\\sum_{j=1}^k \\exp{(z^{(i)}_j)})^{-2}] \\exp{(z^{(i)}_c)} = -\\frac{\\exp{(z^{(i)}_l)}}{\\sum_{j=1}^k \\exp{(z^{(i)}_j)}}\\cdot \\frac{\\exp{(z^{(i)}_c)}}{\\sum_{j=1}^k \\exp{(z^{(i)}_j)}} = - \\mathrm{softmax}(z^{(i)}_l)\\cdot \\mathrm{softmax}(z^{(i)}_c)= - \\hat{y}_l^{(i)} \\hat{y}_c^{(i)}$$\n",
    "\n",
    "Finding the h-th index of $\\nabla_{z^{(i)}} \\mathrm{CE}(y^{(i)}, \\hat{y}^{(i)})$ we have \n",
    "\n",
    "$$-\\begin{pmatrix}\\frac{y_1^{(i)}}{\\hat{y}_l^{(i)}}  \\frac{y_2^{(i)}}{\\hat{y}_2^{(i)}}  \\dots  \\frac{y_h^{(i)}}{\\hat{y}_h^{(i)}} \\dots  \\frac{y_k^{(i)}}{\\hat{y}_k^{(i)}}\\end{pmatrix} \\begin{pmatrix} -\\hat{y}_1^{(i)} \\hat{y}_h^{(i)}\\\\ \\\\ -\\hat{y}_2^{(i)} \\hat{y}_h^{(i)}\\\\ \\\\ \\vdots \\\\ \\\\ \\hat{y}_h^{(i)}(1-\\hat{y}_h^{(i)})\\\\ \\\\ -\\hat{y}_k^{(i)} \\hat{y}_h^{(i)}\\end{pmatrix} = -y_h^{(i)} + \\hat{y}_h^{(i)} \\sum_{j=1}^k y_j^{(i)} = \\hat{y}_h^{(i)} -y_h^{(i)}$$\n",
    "\n",
    "The last equality is due to $\\sum_{j=1}^k y_j^{(i)} = 1$ as $y^{(i)}$ is a one-hot label vector.\n",
    "\n",
    "\n",
    "Since the h-th term of $\\nabla_{z^{(i)}} \\mathrm{CE}(y^{(i)}, \\hat{y}^{(i)}) $ is $\\hat{y}_h^{(i)} -y_h^{(i)}$, We have \n",
    "\n",
    "$$ \\nabla_{z^{(i)}} \\mathrm{CE}(y^{(i)}, \\hat{y}^{(i)}) = \\hat{y}^{(i)} - y^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b\n",
    "Do note that $\\odot$ is called Hadamard product, also known as element-wise product. and $\\cdot$ is the dot product, $x\\cdot y = x^Ty$\n",
    "\n",
    "We need to compute the vector derivative wrt each of the params. Namely, $W1,W2,b1,b2$. We first compute $\\nabla_{W^{[2]}}z^{(i)}$ and $\\nabla_{b2}$\n",
    "\n",
    "$$ \\nabla_{W^{[2]}}z^{(i)} = \\nabla_{W^{[2]}} [{W^{[2]}}^\\top x^{(i)} \\cdot a^{(i)} +b2] = a^{(i)} $$\n",
    "\n",
    "$$ \\nabla_{b2}z^{(i)} = \\nabla_{b2} [{W^{[2]}}^\\top x^{(i)} \\cdot a^{(i)} +b2]= I_{10}$$\n",
    "\n",
    "Since $ \\nabla_{z^{(i)}} \\mathrm{CE}(y^{(i)}, \\hat{y}^{(i)}) = \\hat{y}^{(i)} - y^{(i)}$\n",
    "\n",
    "$$\\nabla_{W^{[2]}} \\mathrm{CE}(y^{(i)}, \\hat{y}^{(i)}) = \\nabla_{z^{(i)}} \\mathrm{CE}(y^{(i)}, \\hat{y}^{(i)}) \\, \\nabla_{W^{[2]}}z^{(i)} = (\\hat{y}^{(i)} - y^{(i)}) a^{(i)T}$$\n",
    "\n",
    "$$\\nabla_{b2} \\mathrm{CE}(y^{(i)}, \\hat{y}^{(i)}) = \\nabla_{z^{(i)}} \\mathrm{CE}(y^{(i)}, \\hat{y}^{(i)}) \\, \\nabla_{b2}z^{(i)} = (\\hat{y}^{(i)} - y^{(i)})$$\n",
    "\n",
    "to find the derivative wrt $W1,b1$, we need to do a bit more work. we first compute\n",
    "\n",
    "\n",
    "$$ \\nabla_{a^{i}}z^{(i)} = \\nabla_{a^{i}} [ {W^{[2]}}^\\top \\cdot a^{(i)} +b2]= {W^{[2]}}^\\top$$\n",
    "\n",
    "Since we know that $a^{(i)} = \\sigma \\left( {W^{[1]}}^\\top x^{(i)}  + b^{[1]} \\right)$. \n",
    "\n",
    "$$ \\nabla_{W^{[1]}} a^{(i)} = \\sigma \\left( a^{(i)} \\right) \\odot \\left(1-\\sigma \\left( a^{(i)} \\right) \\right) x^{(i)T} $$\n",
    "\n",
    "$$ \\nabla_{b^{[1]}} a^{(i)} = \\sigma \\left( a^{(i)} \\right) \\odot \\left(1-\\sigma \\left( a^{(i)} \\right) \\right) $$\n",
    "\n",
    "Combining all together\n",
    "\n",
    "$$ \\nabla_{W^{[1]}}z^{(i)} = \\nabla_{z^{(i)}} \\mathrm{CE}(y^{(i)}, \\hat{y}^{(i)}) \\,\\nabla_{a^{i}}z^{(i)} \\,\\nabla_{W^{[1]}} a^{(i)} =  \\left(\\hat{y}^{(i)} - y^{(i)}\\right) \\cdot {W^{[2]}}^\\top\\sigma \\left( a^{(i)} \\right) \\odot \\left(1-\\sigma \\left( a^{(i)} \\right) \\right) x^{(i)T} $$\n",
    "\n",
    "$$ \\nabla_{b^{[1]}}z^{(i)} = \\nabla_{z^{(i)}} \\mathrm{CE}(y^{(i)}, \\hat{y}^{(i)}) \\,\\nabla_{a^{i}}z^{(i)} \\,\\nabla_{b^{[1]}} a^{(i)} =  \\left(\\hat{y}^{(i)} - y^{(i)}\\right) \\cdot {W^{[2]}}^\\top \\sigma \\left( a^{(i)} \\right) \\odot \\left(1-\\sigma \\left( a^{(i)} \\right) \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nn\n",
    "import importlib\n",
    "import numpy as np\n",
    "importlib.reload(nn)\n",
    "\n",
    "np.random.seed(100)\n",
    "train_data, train_labels = nn.read_data('./images_train.csv', './labels_train.csv')\n",
    "train_labels = nn.one_hot_labels(train_labels)\n",
    "p = np.random.permutation(60000)\n",
    "train_data = train_data[p,:]\n",
    "train_labels = train_labels[p,:]\n",
    "\n",
    "dev_data = train_data[0:10000,:]\n",
    "dev_labels = train_labels[0:10000,:]\n",
    "train_data = train_data[10000:,:]\n",
    "train_labels = train_labels[10000:,:]\n",
    "\n",
    "mean = np.mean(train_data)\n",
    "std = np.std(train_data)\n",
    "train_data = (train_data - mean) / std\n",
    "dev_data = (dev_data - mean) / std\n",
    "\n",
    "test_data, test_labels = nn.read_data('./images_test.csv', './labels_test.csv')\n",
    "test_labels = nn.one_hot_labels(test_labels)\n",
    "test_data = (test_data - mean) / std\n",
    "\n",
    "all_data = {\n",
    "    'train': train_data,\n",
    "    'dev': dev_data,\n",
    "    'test': test_data\n",
    "}\n",
    "\n",
    "all_labels = {\n",
    "    'train': train_labels,\n",
    "    'dev': dev_labels,\n",
    "    'test': test_labels,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "baseline_acc = nn.run_train_test('baseline', all_data, all_labels, \n",
    "                                 nn.backward_prop, num_epochs, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5c\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the graph between the non-regularized and regularized training, we could observe there is a larger gap between train and dev in both accuracy and loss for the baseline model. The performance gap for the regularized model is smaller. The difference suggests the regularization term could help prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
